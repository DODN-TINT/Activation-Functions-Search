# -*- coding: utf-8 -*-
"""Activations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ClsgTt5gdxB8rbXrLQAAYVhD9b72LuzK

<a href="https://colab.research.google.com/github/casperbh96/Activation-Functions-Search/blob/master/Experiment_Activation_Functions.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Example code for search of best activation function
### [----->Inspired by ML From Scratch](https://mlfromscratch.com)
We are going to run comparisons, using mostly Keras and Scikit-Learn. 
The goal is to take some dataset (e.g. MNIST or CIFAR-100 and draw a graph of activations functions vs. loss/accuracy over time).

The outline for this notebook is as follows:
1. [Importing packages and Preprocessing dataset](#1.-Importing-and-preprocessing)
2. [Functions to Visualize Some Data](https://colab.research.google.com/drive/1ClsgTt5gdxB8rbXrLQAAYVhD9b72LuzK#scrollTo=eG2n6VCCZzXj&line=1&uniqifier=1)
3. [Readying functions, hyperparameters etc.](#3.-Readying-functions-and-hyperparameters)
4. [Add New Activation Functions](https://colab.research.google.com/drive/1ClsgTt5gdxB8rbXrLQAAYVhD9b72LuzK#scrollTo=W0UuQA3_kBUh&line=1&uniqifier=1)
5. [Load Tensor Board](https://colab.research.google.com/drive/1ClsgTt5gdxB8rbXrLQAAYVhD9b72LuzK#scrollTo=ueAmdDHzQpeQ&line=1&uniqifier=1)
6. [Fitting data](#6.-Fitting-the-data-with-multiple-activation-functions)
7. [Plotting with Tensor Board](https://colab.research.google.com/drive/1ClsgTt5gdxB8rbXrLQAAYVhD9b72LuzK#scrollTo=Z0K0h4_l970E&line=1&uniqifier=1)
8. [Plotting](#8.-Graph-the-results)

# 1. Importing and preprocessing
"""

# IMPORTS
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import mnist
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, LeakyReLU
from keras.layers.noise import AlphaDropout
from keras.utils.generic_utils import get_custom_objects
from keras import backend as K
from keras.optimizers import Adam
from keras.callbacks import TensorBoard
from time import time

# globals and hyperparameters
LEARNING_RATE = 0.20
CLIP_THRESHOLD = 0.5
ALPHA_DROPOUT = 0.20
DROPOUT_RATE = 0.20
CATEGORIES = 10
VALIDATION_RATIO_SPLIT = 0.20
MAX_COLS_DISPLAY = 10

# PREPROCESSING
def preprocess_data(x_train, y_train, x_test, y_test):

    # Normalizing all images of 28x28 pixels
    # - compare with flattened version of vectors/inputs
    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
    input_shape = (28, 28, 1)

    # Later: Experiment that instead of 28x28x1, make 784,1
    # and probably avoid model.Flatten() later?
    # x_train = x_train.reshape(x_train.shape[0], -1).T
    # x_test = x_test.reshape(x_test.shape[0], -1).T
    # input_shape = (784, 1)

    # Float values for division as original load had dtype=uint8
    # but can be implictly done by /= 255.0 too
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    
    # Normalizing the RGB codes by dividing it to the max RGB value
    x_train /= 255.0
    x_test /= 255.0
    
    # Categorical y values
    y_train = to_categorical(y_train)
    y_test= to_categorical(y_test)
    
    return x_train, y_train, x_test, y_test, input_shape

"""# 2. Display Data"""

def show_images_nolabels(images, cols=1):
    n_images = len(images)
    rows = np.ceil(n_images/cols)
    fig = plt.figure()
    for n, image in enumerate(images):
        a = fig.add_subplot(rows, cols, n + 1)
        plt.imshow(image)
        a.axis('off')

def show_images(images, cols = 1, titles = None):
    """Display a list of images in a single figure with matplotlib.
    Parameters
    ---------
    images: List of np.arrays compatible with plt.imshow.
    cols (Default = 1): Number of columns in figure (number of rows is 
                        set to np.ceil(n_images/float(cols))).
    titles: List of titles corresponding to each image. Must have
            the same length as titles.
    """
    assert((titles is None) or (titles == "default") or (len(images) == len(titles)))
    n_images = len(images)
    rows = np.ceil(n_images/cols)
    fig = plt.figure()
    if (titles is None):
       show_images_nolabels(images, cols)
    else:
       if titles == "default": titles = ['Image (%d)' % i for i in range(1,n_images + 1)]
       for n, (image, title) in enumerate(zip(images, titles)):
          a = fig.add_subplot(rows, cols, n + 1)
          plt.imshow(image)
          a.set_title(title)
          a.axis('off')
    fig.set_size_inches(cols, rows)
    plt.tight_layout()
    plt.show()

"""# 3. Readying functions and hyperparameters"""

# Readying neural network model
def build_CNN(activation,
              dropout_rate,
              optimizer):
    # The following special requirements for SELU (Scaled Exponential LU):
    # Special initializer = 'lecun_normal'
    # AlphaDropout 0.25, not dropout_rate
    # see - arvix 1706.02515 Self-normalizing Neural Network 

    model = Sequential()

    if(activation == 'selu'):
        model.add(Conv2D(32, kernel_size=(3, 3),
                  activation=activation,
                  input_shape=input_shape,
                  kernel_initializer='lecun_normal'))
        model.add(Conv2D(64, (3, 3), activation=activation, 
                         kernel_initializer='lecun_normal'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(AlphaDropout(ALPHA_DROPOUT))
        model.add(Flatten())
        model.add(Dense(128, activation=activation, 
                        kernel_initializer='lecun_normal'))
        model.add(AlphaDropout(dropout_rate))
        model.add(Dense(CATEGORIES, activation='softmax'))
    else:
        model.add(Conv2D(32, kernel_size=(3, 3),
                  activation=activation,
                  input_shape=input_shape))
        model.add(Conv2D(64, (3, 3), activation=activation))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(dropout_rate))
        model.add(Flatten())
        model.add(Dense(128, activation=activation))
        model.add(Dropout(dropout_rate))
        model.add(Dense(CATEGORIES, activation='softmax'))
    # Finalize Model
    model.compile(
        loss='categorical_crossentropy', 
        optimizer=optimizer, 
        metrics=['accuracy']
    )
    model.summary()
    return model

"""# 4. Add New Activation Functions in Keras
## use get_custom_objects
"""

def gelu(x):
  # reference arXiv 1606.084 (revisited Jul 8 2020: GPT-3 benefits from it)
    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))

def add_activation_functions():
    # Add the GELU function to Keras
    get_custom_objects().update({'gelu': Activation(gelu)})
    # Add leaky-relu so we can use it as a string
    get_custom_objects().update({'leaky-relu': Activation(LeakyReLU(alpha=LEARNING_RATE))})
    act_func = ['sigmoid', 'relu', 'elu', 'leaky-relu', 'selu', 'gelu']  
    return act_func

"""# 5. TensorBoard use in Comparing Performance of Various Activation Functions"""

# Commented out IPython magic to ensure Python compatibility.
# Create a TensorBoard logger
# note that everytime this callback function is called, the log_dir is overwritten
# Hence, as best practice, log_dir=<newname with Date and time> as default 
#  and include a RUN NAME label

# Load the TensorBoard notebook extension
# %load_ext tensorboard

"""# 6. Fitting the data with multiple activation functions"""

# LOAD DATA
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_test_raw = x_test
# DISPLAY SOME TRAINING DATA
y_label= [str(i) for i in (y_train[0:50])]
# print(y1label)
show_images(x_train[0:50,:,:], MAX_COLS_DISPLAY, y_label)

# PREPROCESS DATA
x_train, y_train, x_test, y_test, input_shape = preprocess_data(x_train, y_train, x_test, y_test)

# accumulator result in memory (compare with logs)
result = []
# act_func is a list of Strings (name of activation functions)
act_func = ['sigmoid']
act_func = add_activation_functions()

for activation in act_func:
    print('\nTraining with [{0}] activation function\n'.format(activation))
    # tf.keras.callbacks as TensorBoard
    logger = TensorBoard(
            log_dir='/content/drive/My Drive/play_data/activation_function_logs/logs/{}'.format(activation),
            write_graph=True,
            histogram_freq=5)

    model = build_CNN(activation=activation,
                      dropout_rate=DROPOUT_RATE,
                      optimizer=Adam(clipvalue=CLIP_THRESHOLD))
   
    history = model.fit(x_train, y_train,
          validation_split=VALIDATION_RATIO_SPLIT,
          batch_size=128, # 128 is faster, but less accurate. 16/32 recommended
          epochs=50,  # Test run epochs=50 depends on loss asymptotic behavior
          verbose=2,
          callbacks=[logger],
          validation_data=(x_test, y_test))
    
    
    # DISPLAY SOME PARTIAL PREDICTED RESULTS
    print("\nPrediction using [{}]".format(activation))
    # Assumes the process y_test is an array of one hot encoded results
    # the softmax will ensure the max value is the categorization of sample 
    # e.g. [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]
    test_label= [str(y.argmax()) for y in (y_test[0:20])]
    show_images(x_test_raw[0:20,:,:], MAX_COLS_DISPLAY, test_label)

    result.append(history)
    
    K.clear_session()
    del model

print(result)

from google.colab import drive
drive.mount('/content/drive')

"""# 7. Analyze Results with TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir "/content/drive/My Drive/play_data/activation_function_logs/logs"

y=y_train[0:1]
y

# DISPLAY SOME TEST DATA
    print("\nPrediction using [{}]".format(activation))
    test_label= [str(i.argmax()) for i in (y_test[0:20])]
    show_images(x_test_raw[0:20,:,:], MAX_COLS_DISPLAY, test_label)

"""# 8. Graph the results"""

def plot_act_func_results(results, activation_functions = []):
    plt.figure(figsize=(10,10))
    plt.style.use('dark_background')
    
    # Plot validation accuracy values
    for act_func in results:
        plt.plot(act_func.history['val_acc'])
        
    plt.title('Model accuracy')
    plt.ylabel('Test Accuracy')
    plt.xlabel('Epoch')
    plt.legend(activation_functions)
    plt.show()

    # Plot validation loss values
    plt.figure(figsize=(10,10))
    
    for act_func in results:
        plt.plot(act_func.history['val_loss'])
        
    plt.title('Model loss')
    plt.ylabel('Test Loss')
    plt.xlabel('Epoch')
    plt.legend(activation_functions)
    plt.show()
    return

plot_act_func_results(result, act_func)

new_act_arr = act_func[1:]
new_results = result[1:]

plot_act_func_results(new_results, new_act_arr)